{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SLP_on_Solar_data.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNEnuClg0zdwYVALK3Patv3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rashi2011/Rashi-Madhukar/blob/master/Programs%20in%20Tensorflow/SLP_on_Solar_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jh3QSHlxkK-H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install --upgrade tensorflow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGjN3RxDK78D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydqKFFIJLzl_",
        "colab_type": "code",
        "outputId": "6ec69c25-c8bc-4a3f-dce2-91e070957e8d",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "#Downloading the Dataset from local drive\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-21ad3ec8-e095-4915-9dde-f3c084152d8b\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-21ad3ec8-e095-4915-9dde-f3c084152d8b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving sonar.all-data.csv to sonar.all-data.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nbt_hb-lNZL8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Loading the Dataset through pandas as dataframe\n",
        "import io\n",
        "from sklearn.utils import shuffle\n",
        "data= pd.read_csv(io.BytesIO(uploaded['sonar.all-data.csv']))\n",
        "data = shuffle(data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odDo80LbOuqx",
        "colab_type": "code",
        "outputId": "ddfa0740-f076-41a9-bc91-ab39b0a5b2f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Printing the size of the data\n",
        "print(data.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(207, 61)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6Cg6bstO2wi",
        "colab_type": "code",
        "outputId": "e8f2ecbd-9584-4945-d551-e82d738a0045",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#PREPROCESSING\n",
        "X = data.iloc[:,:-1].values\n",
        "Y = data.iloc[:,-1].values\n",
        "#converting Dependent variable to LabelEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "ohe = LabelEncoder()\n",
        "Y = ohe.fit_transform(Y)\n",
        "Y.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(207,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYFpnhUcRg7L",
        "colab_type": "code",
        "outputId": "68c08bbe-6e43-4bb9-c4ff-159fedbedb5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Randomizing the weights\n",
        "w = np.random.randn(60)\n",
        "np.set_printoptions(formatter={'float': '{: 0.2f}'.format})\n",
        "w.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmMWGwWRLozW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#defining all the functions\n",
        "def forward(x,w):\n",
        "  y_pred = np.dot(np.transpose(x),w)\n",
        "  return y_pred\n",
        "\n",
        "def Loss(Y,y_pred):\n",
        "  l = (Y-y_pred)\n",
        "  return l\n",
        "\n",
        "def gradient(X,l):\n",
        "  return l*X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9Pv8HmTLvM4",
        "colab_type": "code",
        "outputId": "64ca5274-fff1-4a2f-eeb2-aa995facff1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Training the data\n",
        "epochs = 0\n",
        "lr = .01\n",
        "while epochs < 70:\n",
        "  i = 0\n",
        "  for x in X:\n",
        "    y_pred = forward(x,w)\n",
        "    l = Loss(Y[i],y_pred)\n",
        "    i+=1\n",
        "    dw = gradient(x,l)\n",
        "    #updating the weight\n",
        "    w  += lr*dw\n",
        "  print(\"epochs = {} , loss = {:.6f}\".format(epochs+1,l) )\n",
        "  epochs+=1\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epochs = 1 , loss = 0.023483\n",
            "epochs = 2 , loss = 0.023418\n",
            "epochs = 3 , loss = 0.023354\n",
            "epochs = 4 , loss = 0.023289\n",
            "epochs = 5 , loss = 0.023224\n",
            "epochs = 6 , loss = 0.023159\n",
            "epochs = 7 , loss = 0.023094\n",
            "epochs = 8 , loss = 0.023029\n",
            "epochs = 9 , loss = 0.022964\n",
            "epochs = 10 , loss = 0.022899\n",
            "epochs = 11 , loss = 0.022834\n",
            "epochs = 12 , loss = 0.022769\n",
            "epochs = 13 , loss = 0.022704\n",
            "epochs = 14 , loss = 0.022638\n",
            "epochs = 15 , loss = 0.022573\n",
            "epochs = 16 , loss = 0.022507\n",
            "epochs = 17 , loss = 0.022442\n",
            "epochs = 18 , loss = 0.022376\n",
            "epochs = 19 , loss = 0.022311\n",
            "epochs = 20 , loss = 0.022245\n",
            "epochs = 21 , loss = 0.022179\n",
            "epochs = 22 , loss = 0.022113\n",
            "epochs = 23 , loss = 0.022048\n",
            "epochs = 24 , loss = 0.021982\n",
            "epochs = 25 , loss = 0.021916\n",
            "epochs = 26 , loss = 0.021850\n",
            "epochs = 27 , loss = 0.021784\n",
            "epochs = 28 , loss = 0.021718\n",
            "epochs = 29 , loss = 0.021651\n",
            "epochs = 30 , loss = 0.021585\n",
            "epochs = 31 , loss = 0.021519\n",
            "epochs = 32 , loss = 0.021453\n",
            "epochs = 33 , loss = 0.021387\n",
            "epochs = 34 , loss = 0.021320\n",
            "epochs = 35 , loss = 0.021254\n",
            "epochs = 36 , loss = 0.021188\n",
            "epochs = 37 , loss = 0.021121\n",
            "epochs = 38 , loss = 0.021055\n",
            "epochs = 39 , loss = 0.020989\n",
            "epochs = 40 , loss = 0.020922\n",
            "epochs = 41 , loss = 0.020856\n",
            "epochs = 42 , loss = 0.020789\n",
            "epochs = 43 , loss = 0.020723\n",
            "epochs = 44 , loss = 0.020656\n",
            "epochs = 45 , loss = 0.020590\n",
            "epochs = 46 , loss = 0.020523\n",
            "epochs = 47 , loss = 0.020456\n",
            "epochs = 48 , loss = 0.020390\n",
            "epochs = 49 , loss = 0.020323\n",
            "epochs = 50 , loss = 0.020257\n",
            "epochs = 51 , loss = 0.020190\n",
            "epochs = 52 , loss = 0.020123\n",
            "epochs = 53 , loss = 0.020057\n",
            "epochs = 54 , loss = 0.019990\n",
            "epochs = 55 , loss = 0.019923\n",
            "epochs = 56 , loss = 0.019857\n",
            "epochs = 57 , loss = 0.019790\n",
            "epochs = 58 , loss = 0.019724\n",
            "epochs = 59 , loss = 0.019657\n",
            "epochs = 60 , loss = 0.019590\n",
            "epochs = 61 , loss = 0.019524\n",
            "epochs = 62 , loss = 0.019457\n",
            "epochs = 63 , loss = 0.019390\n",
            "epochs = 64 , loss = 0.019324\n",
            "epochs = 65 , loss = 0.019257\n",
            "epochs = 66 , loss = 0.019190\n",
            "epochs = 67 , loss = 0.019124\n",
            "epochs = 68 , loss = 0.019057\n",
            "epochs = 69 , loss = 0.018991\n",
            "epochs = 70 , loss = 0.018924\n",
            "epochs = 71 , loss = 0.018857\n",
            "epochs = 72 , loss = 0.018791\n",
            "epochs = 73 , loss = 0.018724\n",
            "epochs = 74 , loss = 0.018658\n",
            "epochs = 75 , loss = 0.018591\n",
            "epochs = 76 , loss = 0.018525\n",
            "epochs = 77 , loss = 0.018458\n",
            "epochs = 78 , loss = 0.018392\n",
            "epochs = 79 , loss = 0.018325\n",
            "epochs = 80 , loss = 0.018259\n",
            "epochs = 81 , loss = 0.018193\n",
            "epochs = 82 , loss = 0.018126\n",
            "epochs = 83 , loss = 0.018060\n",
            "epochs = 84 , loss = 0.017994\n",
            "epochs = 85 , loss = 0.017927\n",
            "epochs = 86 , loss = 0.017861\n",
            "epochs = 87 , loss = 0.017795\n",
            "epochs = 88 , loss = 0.017729\n",
            "epochs = 89 , loss = 0.017662\n",
            "epochs = 90 , loss = 0.017596\n",
            "epochs = 91 , loss = 0.017530\n",
            "epochs = 92 , loss = 0.017464\n",
            "epochs = 93 , loss = 0.017398\n",
            "epochs = 94 , loss = 0.017332\n",
            "epochs = 95 , loss = 0.017266\n",
            "epochs = 96 , loss = 0.017200\n",
            "epochs = 97 , loss = 0.017134\n",
            "epochs = 98 , loss = 0.017068\n",
            "epochs = 99 , loss = 0.017002\n",
            "epochs = 100 , loss = 0.016936\n",
            "epochs = 101 , loss = 0.016870\n",
            "epochs = 102 , loss = 0.016805\n",
            "epochs = 103 , loss = 0.016739\n",
            "epochs = 104 , loss = 0.016673\n",
            "epochs = 105 , loss = 0.016608\n",
            "epochs = 106 , loss = 0.016542\n",
            "epochs = 107 , loss = 0.016476\n",
            "epochs = 108 , loss = 0.016411\n",
            "epochs = 109 , loss = 0.016345\n",
            "epochs = 110 , loss = 0.016280\n",
            "epochs = 111 , loss = 0.016215\n",
            "epochs = 112 , loss = 0.016149\n",
            "epochs = 113 , loss = 0.016084\n",
            "epochs = 114 , loss = 0.016019\n",
            "epochs = 115 , loss = 0.015953\n",
            "epochs = 116 , loss = 0.015888\n",
            "epochs = 117 , loss = 0.015823\n",
            "epochs = 118 , loss = 0.015758\n",
            "epochs = 119 , loss = 0.015693\n",
            "epochs = 120 , loss = 0.015628\n",
            "epochs = 121 , loss = 0.015563\n",
            "epochs = 122 , loss = 0.015498\n",
            "epochs = 123 , loss = 0.015433\n",
            "epochs = 124 , loss = 0.015369\n",
            "epochs = 125 , loss = 0.015304\n",
            "epochs = 126 , loss = 0.015239\n",
            "epochs = 127 , loss = 0.015175\n",
            "epochs = 128 , loss = 0.015110\n",
            "epochs = 129 , loss = 0.015046\n",
            "epochs = 130 , loss = 0.014981\n",
            "epochs = 131 , loss = 0.014917\n",
            "epochs = 132 , loss = 0.014852\n",
            "epochs = 133 , loss = 0.014788\n",
            "epochs = 134 , loss = 0.014724\n",
            "epochs = 135 , loss = 0.014660\n",
            "epochs = 136 , loss = 0.014596\n",
            "epochs = 137 , loss = 0.014531\n",
            "epochs = 138 , loss = 0.014467\n",
            "epochs = 139 , loss = 0.014403\n",
            "epochs = 140 , loss = 0.014340\n",
            "epochs = 141 , loss = 0.014276\n",
            "epochs = 142 , loss = 0.014212\n",
            "epochs = 143 , loss = 0.014148\n",
            "epochs = 144 , loss = 0.014085\n",
            "epochs = 145 , loss = 0.014021\n",
            "epochs = 146 , loss = 0.013957\n",
            "epochs = 147 , loss = 0.013894\n",
            "epochs = 148 , loss = 0.013831\n",
            "epochs = 149 , loss = 0.013767\n",
            "epochs = 150 , loss = 0.013704\n",
            "epochs = 151 , loss = 0.013641\n",
            "epochs = 152 , loss = 0.013578\n",
            "epochs = 153 , loss = 0.013514\n",
            "epochs = 154 , loss = 0.013451\n",
            "epochs = 155 , loss = 0.013388\n",
            "epochs = 156 , loss = 0.013326\n",
            "epochs = 157 , loss = 0.013263\n",
            "epochs = 158 , loss = 0.013200\n",
            "epochs = 159 , loss = 0.013137\n",
            "epochs = 160 , loss = 0.013075\n",
            "epochs = 161 , loss = 0.013012\n",
            "epochs = 162 , loss = 0.012949\n",
            "epochs = 163 , loss = 0.012887\n",
            "epochs = 164 , loss = 0.012825\n",
            "epochs = 165 , loss = 0.012762\n",
            "epochs = 166 , loss = 0.012700\n",
            "epochs = 167 , loss = 0.012638\n",
            "epochs = 168 , loss = 0.012576\n",
            "epochs = 169 , loss = 0.012514\n",
            "epochs = 170 , loss = 0.012452\n",
            "epochs = 171 , loss = 0.012390\n",
            "epochs = 172 , loss = 0.012328\n",
            "epochs = 173 , loss = 0.012266\n",
            "epochs = 174 , loss = 0.012205\n",
            "epochs = 175 , loss = 0.012143\n",
            "epochs = 176 , loss = 0.012081\n",
            "epochs = 177 , loss = 0.012020\n",
            "epochs = 178 , loss = 0.011959\n",
            "epochs = 179 , loss = 0.011897\n",
            "epochs = 180 , loss = 0.011836\n",
            "epochs = 181 , loss = 0.011775\n",
            "epochs = 182 , loss = 0.011714\n",
            "epochs = 183 , loss = 0.011653\n",
            "epochs = 184 , loss = 0.011592\n",
            "epochs = 185 , loss = 0.011531\n",
            "epochs = 186 , loss = 0.011470\n",
            "epochs = 187 , loss = 0.011409\n",
            "epochs = 188 , loss = 0.011348\n",
            "epochs = 189 , loss = 0.011288\n",
            "epochs = 190 , loss = 0.011227\n",
            "epochs = 191 , loss = 0.011167\n",
            "epochs = 192 , loss = 0.011106\n",
            "epochs = 193 , loss = 0.011046\n",
            "epochs = 194 , loss = 0.010986\n",
            "epochs = 195 , loss = 0.010926\n",
            "epochs = 196 , loss = 0.010866\n",
            "epochs = 197 , loss = 0.010806\n",
            "epochs = 198 , loss = 0.010746\n",
            "epochs = 199 , loss = 0.010686\n",
            "epochs = 200 , loss = 0.010626\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHC-Xvu_bEB6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Step function \n",
        "def step(X,Theta):\n",
        "  if X<Theta:\n",
        "    return 0\n",
        "  else:\n",
        "    return 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FX4sDuCvbC1_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#prediction using the updated weights\n",
        "pred = np.dot(X,w)\n",
        "mean = np.mean(pred)\n",
        "y = []\n",
        "for j in pred:\n",
        "  y.append(step(j,mean))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGXVWR4wcUwT",
        "colab_type": "code",
        "outputId": "c4fb0693-587f-45de-fc30-d34ee9f61c11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#Performance Measure\n",
        "from sklearn.metrics import confusion_matrix,f1_score\n",
        "cm = confusion_matrix(Y,y)\n",
        "print(\"F1_Score: \",f1_score(Y,y))\n",
        "print(\"Confusion_Matrix\",cm)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1_Score:  0.8426395939086294\n",
            "Confusion_Matrix [[93 18]\n",
            " [13 83]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQiFKRJNTHHm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}