{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "n_layered_MLP_scratch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOaTL2IYDwzF8rPErD8cZ4H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rashi2011/Rashi-Madhukar/blob/master/Multiple%20Level%20Perceptron/n_layered_MLP_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65IJ9cIzK0x3"
      },
      "source": [
        "#import Libraries\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "aXYjVfJEK9Et",
        "outputId": "d5763387-ef54-43ac-e924-b24cff5091d7"
      },
      "source": [
        "#import Dataset\r\n",
        "import io\r\n",
        "from google.colab import files\r\n",
        "uploaded=files.upload()\r\n",
        "dataset = pd.read_csv(io.BytesIO(uploaded[\"Social_Network_Ads.csv\"]))\r\n",
        "\r\n",
        "x = dataset.iloc[:,1:4].values\r\n",
        "y = dataset.iloc[:,4].values\r\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9d2d260f-65aa-46be-8895-172266fc8d9b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9d2d260f-65aa-46be-8895-172266fc8d9b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving Social_Network_Ads.csv to Social_Network_Ads.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_ppUzvELBGe",
        "outputId": "653c28aa-9dc7-4531-b407-f029e2bd0340"
      },
      "source": [
        "#converting column sex to numbers\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "lb=LabelEncoder()\r\n",
        "x[:,0]=lb.fit_transform(x[:,0])\r\n",
        "\r\n",
        "#Standard Scaling\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "sc=StandardScaler()\r\n",
        "x=sc.fit_transform(x)\r\n",
        "\r\n",
        "\r\n",
        "print(x.shape,y.shape)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3, 400) (400,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suXGiBNULFOO"
      },
      "source": [
        "#Initialize Parameters\r\n",
        "def initialize_param(layers_dims):\r\n",
        "  np.random.seed(105)\r\n",
        "  parameters = {}                #empty dictionary\r\n",
        "  L = len(layers_dims)           # 4-1 no of layers in the network\r\n",
        "  for l in range(1,L):   #1,2,3\r\n",
        "    parameters[\"W\" + str(l)]= np.round(np.random.randn(layers_dims[l],layers_dims[l-1]),2)\r\n",
        "    parameters[\"b\"+ str(l)]= np.round(np.zeros((layers_dims[l],1)),2)\r\n",
        "\r\n",
        "  return parameters"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klCZ-3BF5PWz",
        "outputId": "eda45e0f-360e-496a-df94-2bac4f2bac05"
      },
      "source": [
        "\r\n",
        "layers_dims = [3,5,3,1]\r\n",
        "parameters = initialize_param(layers_dims)\r\n",
        "#print(parameters)             #Dictionary\r\n",
        "print(\"W1 = \"+ str(parameters[\"W1\"]))\r\n",
        "print(parameters[\"W1\"].shape)\r\n",
        "print(\"b1 = \"+ str(parameters[\"b1\"]))\r\n",
        "print(\"W2 = \"+ str(parameters[\"W2\"]))\r\n",
        "print(\"b2 = \"+ str(parameters[\"b2\"]))\r\n",
        "print(\"W3 = \"+ str(parameters[\"W3\"]))\r\n",
        "print(\"b3 = \"+ str(parameters[\"b3\"]))\r\n",
        "\r\n"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W1 = [[-0.25 -0.61 -0.73]\n",
            " [ 0.06  1.01 -0.13]\n",
            " [-1.53  0.47 -0.56]\n",
            " [-0.2   1.7   2.57]\n",
            " [ 0.56  0.95 -1.41]]\n",
            "(5, 3)\n",
            "b1 = [[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "W2 = [[ 1.42 -1.29  0.14  0.69 -0.3 ]\n",
            " [-1.58  0.29  0.89 -1.73 -0.09]\n",
            " [ 1.45  1.84  1.55  0.62  0.66]]\n",
            "b2 = [[0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "W3 = [[ 0.14  2.09 -1.29]]\n",
            "b3 = [[0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDkTo-leLGF6"
      },
      "source": [
        "#Activation Function\r\n",
        "def sigmoid(z):\r\n",
        "  A = 1/(1+np.exp(-z))\r\n",
        "  return A\r\n",
        "\r\n",
        "def relu(z):\r\n",
        "  A = np.maximum(0,z)\r\n",
        "  return A\r\n",
        "\r\n",
        "def sign_f(AL,mean):\r\n",
        "    if AL <= mean:\r\n",
        "      return 0\r\n",
        "    else:\r\n",
        "      return 1"
      ],
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBVjmGZKLI4n"
      },
      "source": [
        "#forward\r\n",
        "def forward(A, W, b):\r\n",
        "  \r\n",
        "  Z = np.dot(W,A) +b\r\n",
        "  #print(\"Z.shape = \",Z)       \r\n",
        "  \r\n",
        "  return Z"
      ],
      "execution_count": 298,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcK4ZcHnLLbg"
      },
      "source": [
        "#Activations forward\r\n",
        "def activation_forward(A_prev, W, b ,activation):\r\n",
        "  if activation == \"sigmoid\":\r\n",
        "    #print(\"W,b\",W.shape,b.shape)\r\n",
        "    Z = forward(A_prev,W,b)\r\n",
        "    A  = sigmoid(Z)\r\n",
        "    \r\n",
        "\r\n",
        "  elif activation == \"tanh\":\r\n",
        "    #print(\"W,b\",W.shape,b.shape)\r\n",
        "    Z = forward(A_prev,W,b)\r\n",
        "    A  =  np.tanh(Z)\r\n",
        "    #print(\"Ashapes\",A)\r\n",
        "          \r\n",
        "  return A,Z\r\n"
      ],
      "execution_count": 299,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZhE3rCbLOIt"
      },
      "source": [
        "#Forward Propagation\r\n",
        "def forward_model(X,parameters):    \r\n",
        "  cache = {}  #empty dictionary    \r\n",
        "  A = X\r\n",
        "  L = len(parameters)//2     #3\r\n",
        " \r\n",
        "  for l in range(1,L):  #1,2\r\n",
        "    A_prev =A\r\n",
        "    A,cache[\"Z\"+str(l)] = activation_forward(A_prev, parameters['W'+ str(l)], parameters['b'+ str(l)], activation = \"tanh\")  #i = 1,i = 2\r\n",
        "    cache[\"A\"+str(l)] = A\r\n",
        "    \r\n",
        "  #l=3\r\n",
        "  l=L\r\n",
        "  AL,cache[\"Z\"+str(l)] = activation_forward(cache[\"A\"+str(l-1)], parameters[\"W\"+str(l)], parameters[\"b\"+str(l)], activation = \"sigmoid\")  #i = 3\r\n",
        "  cache[\"A\"+str(l)] = AL\r\n",
        "  \r\n",
        "  return AL,cache"
      ],
      "execution_count": 300,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0Zz3OiCLQ78"
      },
      "source": [
        "#Cost Function\r\n",
        "def compute_cost(y_pred,Y):\r\n",
        "  \r\n",
        "  m = len(Y)\r\n",
        "  #print(m)\r\n",
        "  cost = -(1/m)*np.sum((Y*np.log(y_pred) + (1-Y)*np.log(1-y_pred)))\r\n",
        "  #logprobs = np.multiply(np.log(AL),Y)\r\n",
        "  #cost = np.sum(logprobs,axis = 1,keepdims = True)\r\n",
        "  \r\n",
        "  return cost"
      ],
      "execution_count": 301,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqSgWq9xCWVZ"
      },
      "source": [
        "\r\n",
        "def sigmoid_backward(dA,AL):\r\n",
        "  \r\n",
        "  dZ = np.multiply(AL,(1-AL))\r\n",
        "  Loss = np.multiply(dZ,AL)             #(1x207)\r\n",
        "  return Loss\r\n",
        "\r\n",
        "\r\n",
        "def tanh_backward(dA,AL):\r\n",
        "  dZ = 1-np.power(dA,2)\r\n",
        "\r\n",
        "  #print(dZ.shape)               #(5x207)\r\n",
        "  dZ = np.multiply(dZ,AL)\r\n",
        "  Loss = np.multiply(dA,dZ)\r\n",
        "  return Loss\r\n",
        "\r\n",
        "def relu_backward(dA,AL):\r\n",
        "  dZ= np.zeros((AL.shape))\r\n",
        "  for i in range(AL.shape[0]):\r\n",
        "    for j in range(AL.shape[1]):\r\n",
        "        \r\n",
        "        if AL[i][j] <= 0:\r\n",
        "          dZ[i][j] = dZ[i][j]+ 0\r\n",
        "        else:\r\n",
        "          dZ[i][j] =dZ[i][j] + 1\r\n",
        "  #print(dZ.shape)               #(5x207)\r\n",
        "  dZ = np.multiply(dZ,AL)\r\n",
        "  Loss = np.multiply(dA,dZ)\r\n",
        "\r\n",
        "  return Loss"
      ],
      "execution_count": 317,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_H0FBnCQCTxj"
      },
      "source": [
        "#Backward \r\n",
        "def backward(dA,W,A_prev,Loss): \r\n",
        "  m = dA.shape[1]            \r\n",
        "  dW = (1/m)*np.dot(Loss,A_prev.T)   #(1x3)\r\n",
        "  dA_ = np.dot(W.T,dA)       #3x207\r\n",
        "  #print(\"dA_\",dA_.shape)\r\n",
        "  db = (1/m)*np.sum(Loss,axis = 1,keepdims = True) \r\n",
        "\r\n",
        "  return dA_,dW,db"
      ],
      "execution_count": 318,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewz8Eq2WCRln"
      },
      "source": [
        "#Activation backward\r\n",
        "def activation_backward(dA,W,A_prev,AL ,activation):\r\n",
        "  \r\n",
        "  if activation == \"tanh\":\r\n",
        "    Loss = tanh_backward(dA, AL )\r\n",
        "    dA_,dW ,db = backward(dA,W,A_prev,Loss)\r\n",
        "  elif activation == \"sigmoid\":\r\n",
        "    Loss = sigmoid_backward(dA,AL)\r\n",
        "    dA_,dW ,db = backward(dA,W,A_prev,Loss)\r\n",
        "\r\n",
        "  return dA_,dW,db"
      ],
      "execution_count": 319,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTIg7gczCIXE"
      },
      "source": [
        "#backward Model\r\n",
        "def backward_model(AL,X,Y, cache,parameters):\r\n",
        "  \r\n",
        "  grads = {}           #empty dictionary\r\n",
        "  L = int(len(parameters)/2)              #3    #no of layers  \r\n",
        "  #print(L)  #no of layers\r\n",
        "  m = len(Y)                #no of training examples\r\n",
        "  Y = Y.reshape(AL.shape)       #to ensure shape of Y = AL\r\n",
        "  cache[\"A\" + str(0)] = X\r\n",
        "  #intialization of backward propagation\r\n",
        "  dAL = AL- Y           #dA3\r\n",
        "  #print(\"dAL\",dAL.shape)\r\n",
        "  #Sigmoid (for layer L = L-1 )       #L = 3\r\n",
        "  \r\n",
        "  grads[\"dA\"+ str(L-1)],grads[\"dW\"+ str(L)] ,grads[\"db\"+ str(L)]  = activation_backward(dAL,parameters[\"W\"+str(L)],cache[\"A\" + str(L-1)],cache[\"A\"+ str(L)] ,activation = \"sigmoid\")\r\n",
        "\r\n",
        "  # for layer (L =L-2 to  L=0 ) \r\n",
        "  for l in reversed(range(L-1)):    #L-1 = 2, #(0 to 2) # means l = 0,1 # reversing it  l = 1,0\r\n",
        "    if l!=0:\r\n",
        "      grads[\"dA\"+ str(l)],grads[\"dW\"+ str(l+1)] ,grads[\"db\"+ str(l+1)]  = activation_backward(grads[\"dA\"+ str(l+1)], parameters[\"W\"+ str(l+1)],cache[\"A\"+str(l)],cache[\"A\"+str(l+1)] ,activation = \"tanh\")\r\n",
        "\r\n",
        "    else:\r\n",
        "      _,grads[\"dW\"+ str(l+1)] ,grads[\"db\"+ str(l+1)]  = activation_backward(grads[\"dA\"+ str(l+1)], parameters[\"W\"+ str(l+1)],cache[\"A\"+ str(l)] ,cache[\"A\"+str(l+1)],activation = \"tanh\")\r\n",
        "  return grads"
      ],
      "execution_count": 320,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJU9r491LmGf"
      },
      "source": [
        "#Update Parameters\r\n",
        "def update_param(parameters,grads,learning_rate):\r\n",
        "  L = len(parameters)//2   #3\r\n",
        "  for l in range(L):  #0,1,2\r\n",
        "    parameters[\"W\"+str(l+1)] = parameters[\"W\"+str(l+1)] - (learning_rate*(grads[\"dW\"+ str(l+1)]))\r\n",
        "    parameters[\"b\"+str(l+1)] = parameters[\"b\"+str(l+1)] - (learning_rate*(grads[\"db\"+ str(l+1)]))\r\n",
        "\r\n",
        "  return parameters"
      ],
      "execution_count": 321,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-klafjLHLpI_"
      },
      "source": [
        "from sklearn.metrics import f1_score\r\n",
        "#NN Model\r\n",
        "def L_layer_model(X, Y, layers_dims, learning_rate , num_iterations):\r\n",
        "  costs = []                #empty list\r\n",
        "  acc = []\r\n",
        "  parameters = initialize_param(layers_dims)\r\n",
        "  L=len(parameters)//2\r\n",
        "  for i in range(0,num_iterations):\r\n",
        "    #Forward Propagation(relu to sigmoid)\r\n",
        "    \r\n",
        "    AL,cache = forward_model(X,parameters)\r\n",
        "    \r\n",
        "    #compute cost\r\n",
        "    cost = compute_cost(AL,Y)\r\n",
        "    \r\n",
        "    #Backward Propagation\r\n",
        "    grads = backward_model(AL,X,Y, cache,parameters)\r\n",
        "    \r\n",
        "    #Update Parameters\r\n",
        "    parameters = update_param(parameters,grads,learning_rate)\r\n",
        "    \r\n",
        "\r\n",
        "    #Print the cost on every 100th training examples\r\n",
        "    \r\n",
        "    print (\"Cost after iteration %i: %f\" %(i, cost))\r\n",
        "    \r\n",
        "    #Converting to y_pred\r\n",
        "    \r\n",
        "    y_pred = []\r\n",
        "    #print(AL.shape,Y.shape)\r\n",
        "\r\n",
        "    \r\n",
        "    for j in range(AL.shape[1]):\r\n",
        "      y_pred.append(sign_f(AL[0][j],np.mean(AL)))\r\n",
        "    #y_pred = np.asmatrix(y_pred)\r\n",
        "    #print(y_pred.shape)\r\n",
        "    #print(y_pred)\r\n",
        "  \r\n",
        "    costs.append(cost)\r\n",
        "    #Accuracy\r\n",
        "  \r\n",
        "    print(\"Accuracy = {:.2f}\".format(f1_score(Y,y_pred)))\r\n",
        "    acc.append(f1_score(Y, y_pred,average = 'micro'))\r\n",
        "         \r\n",
        "  \r\n",
        "  return parameters,costs,acc\r\n"
      ],
      "execution_count": 343,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeqEOA79LsML",
        "outputId": "ebd2ab25-393d-4383-ba6e-ccef82f9a308"
      },
      "source": [
        "layers_dims = [3,10,1]\r\n",
        "parameters,cost,acc = L_layer_model(x, y, layers_dims, 0.01,num_iterations = 500)"
      ],
      "execution_count": 344,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost after iteration 0: 0.746627\n",
            "Accuracy = 0.44\n",
            "Cost after iteration 1: 0.747424\n",
            "Accuracy = 0.44\n",
            "Cost after iteration 2: 0.748227\n",
            "Accuracy = 0.45\n",
            "Cost after iteration 3: 0.749034\n",
            "Accuracy = 0.45\n",
            "Cost after iteration 4: 0.749846\n",
            "Accuracy = 0.45\n",
            "Cost after iteration 5: 0.750663\n",
            "Accuracy = 0.45\n",
            "Cost after iteration 6: 0.751484\n",
            "Accuracy = 0.44\n",
            "Cost after iteration 7: 0.752308\n",
            "Accuracy = 0.44\n",
            "Cost after iteration 8: 0.753136\n",
            "Accuracy = 0.44\n",
            "Cost after iteration 9: 0.753967\n",
            "Accuracy = 0.44\n",
            "Cost after iteration 10: 0.754800\n",
            "Accuracy = 0.44\n",
            "Cost after iteration 11: 0.755636\n",
            "Accuracy = 0.44\n",
            "Cost after iteration 12: 0.756474\n",
            "Accuracy = 0.44\n",
            "Cost after iteration 13: 0.757313\n",
            "Accuracy = 0.45\n",
            "Cost after iteration 14: 0.758154\n",
            "Accuracy = 0.45\n",
            "Cost after iteration 15: 0.758995\n",
            "Accuracy = 0.45\n",
            "Cost after iteration 16: 0.759837\n",
            "Accuracy = 0.45\n",
            "Cost after iteration 17: 0.760679\n",
            "Accuracy = 0.45\n",
            "Cost after iteration 18: 0.761521\n",
            "Accuracy = 0.45\n",
            "Cost after iteration 19: 0.762362\n",
            "Accuracy = 0.45\n",
            "Cost after iteration 20: 0.763203\n",
            "Accuracy = 0.45\n",
            "Cost after iteration 21: 0.764042\n",
            "Accuracy = 0.45\n",
            "Cost after iteration 22: 0.764879\n",
            "Accuracy = 0.45\n",
            "Cost after iteration 23: 0.765715\n",
            "Accuracy = 0.45\n",
            "Cost after iteration 24: 0.766548\n",
            "Accuracy = 0.45\n",
            "Cost after iteration 25: 0.767379\n",
            "Accuracy = 0.45\n",
            "Cost after iteration 26: 0.768207\n",
            "Accuracy = 0.45\n",
            "Cost after iteration 27: 0.769032\n",
            "Accuracy = 0.46\n",
            "Cost after iteration 28: 0.769854\n",
            "Accuracy = 0.46\n",
            "Cost after iteration 29: 0.770673\n",
            "Accuracy = 0.46\n",
            "Cost after iteration 30: 0.771487\n",
            "Accuracy = 0.46\n",
            "Cost after iteration 31: 0.772298\n",
            "Accuracy = 0.46\n",
            "Cost after iteration 32: 0.773104\n",
            "Accuracy = 0.46\n",
            "Cost after iteration 33: 0.773906\n",
            "Accuracy = 0.46\n",
            "Cost after iteration 34: 0.774704\n",
            "Accuracy = 0.46\n",
            "Cost after iteration 35: 0.775497\n",
            "Accuracy = 0.46\n",
            "Cost after iteration 36: 0.776285\n",
            "Accuracy = 0.46\n",
            "Cost after iteration 37: 0.777069\n",
            "Accuracy = 0.46\n",
            "Cost after iteration 38: 0.777848\n",
            "Accuracy = 0.46\n",
            "Cost after iteration 39: 0.778621\n",
            "Accuracy = 0.45\n",
            "Cost after iteration 40: 0.779390\n",
            "Accuracy = 0.46\n",
            "Cost after iteration 41: 0.780154\n",
            "Accuracy = 0.46\n",
            "Cost after iteration 42: 0.780913\n",
            "Accuracy = 0.46\n",
            "Cost after iteration 43: 0.781667\n",
            "Accuracy = 0.46\n",
            "Cost after iteration 44: 0.782416\n",
            "Accuracy = 0.47\n",
            "Cost after iteration 45: 0.783161\n",
            "Accuracy = 0.47\n",
            "Cost after iteration 46: 0.783901\n",
            "Accuracy = 0.47\n",
            "Cost after iteration 47: 0.784636\n",
            "Accuracy = 0.47\n",
            "Cost after iteration 48: 0.785366\n",
            "Accuracy = 0.47\n",
            "Cost after iteration 49: 0.786092\n",
            "Accuracy = 0.48\n",
            "Cost after iteration 50: 0.786814\n",
            "Accuracy = 0.49\n",
            "Cost after iteration 51: 0.787532\n",
            "Accuracy = 0.50\n",
            "Cost after iteration 52: 0.788246\n",
            "Accuracy = 0.50\n",
            "Cost after iteration 53: 0.788956\n",
            "Accuracy = 0.50\n",
            "Cost after iteration 54: 0.789663\n",
            "Accuracy = 0.50\n",
            "Cost after iteration 55: 0.790366\n",
            "Accuracy = 0.50\n",
            "Cost after iteration 56: 0.791066\n",
            "Accuracy = 0.50\n",
            "Cost after iteration 57: 0.791763\n",
            "Accuracy = 0.50\n",
            "Cost after iteration 58: 0.792458\n",
            "Accuracy = 0.50\n",
            "Cost after iteration 59: 0.793150\n",
            "Accuracy = 0.50\n",
            "Cost after iteration 60: 0.793840\n",
            "Accuracy = 0.50\n",
            "Cost after iteration 61: 0.794528\n",
            "Accuracy = 0.50\n",
            "Cost after iteration 62: 0.795214\n",
            "Accuracy = 0.50\n",
            "Cost after iteration 63: 0.795899\n",
            "Accuracy = 0.50\n",
            "Cost after iteration 64: 0.796583\n",
            "Accuracy = 0.50\n",
            "Cost after iteration 65: 0.797265\n",
            "Accuracy = 0.50\n",
            "Cost after iteration 66: 0.797947\n",
            "Accuracy = 0.50\n",
            "Cost after iteration 67: 0.798629\n",
            "Accuracy = 0.50\n",
            "Cost after iteration 68: 0.799310\n",
            "Accuracy = 0.50\n",
            "Cost after iteration 69: 0.799991\n",
            "Accuracy = 0.50\n",
            "Cost after iteration 70: 0.800673\n",
            "Accuracy = 0.50\n",
            "Cost after iteration 71: 0.801355\n",
            "Accuracy = 0.50\n",
            "Cost after iteration 72: 0.802038\n",
            "Accuracy = 0.50\n",
            "Cost after iteration 73: 0.802722\n",
            "Accuracy = 0.50\n",
            "Cost after iteration 74: 0.803408\n",
            "Accuracy = 0.50\n",
            "Cost after iteration 75: 0.804095\n",
            "Accuracy = 0.50\n",
            "Cost after iteration 76: 0.804783\n",
            "Accuracy = 0.50\n",
            "Cost after iteration 77: 0.805474\n",
            "Accuracy = 0.50\n",
            "Cost after iteration 78: 0.806167\n",
            "Accuracy = 0.50\n",
            "Cost after iteration 79: 0.806862\n",
            "Accuracy = 0.50\n",
            "Cost after iteration 80: 0.807560\n",
            "Accuracy = 0.50\n",
            "Cost after iteration 81: 0.808260\n",
            "Accuracy = 0.50\n",
            "Cost after iteration 82: 0.808964\n",
            "Accuracy = 0.50\n",
            "Cost after iteration 83: 0.809671\n",
            "Accuracy = 0.50\n",
            "Cost after iteration 84: 0.810381\n",
            "Accuracy = 0.50\n",
            "Cost after iteration 85: 0.811095\n",
            "Accuracy = 0.50\n",
            "Cost after iteration 86: 0.811812\n",
            "Accuracy = 0.50\n",
            "Cost after iteration 87: 0.812533\n",
            "Accuracy = 0.50\n",
            "Cost after iteration 88: 0.813258\n",
            "Accuracy = 0.50\n",
            "Cost after iteration 89: 0.813988\n",
            "Accuracy = 0.50\n",
            "Cost after iteration 90: 0.814721\n",
            "Accuracy = 0.50\n",
            "Cost after iteration 91: 0.815460\n",
            "Accuracy = 0.50\n",
            "Cost after iteration 92: 0.816202\n",
            "Accuracy = 0.50\n",
            "Cost after iteration 93: 0.816950\n",
            "Accuracy = 0.50\n",
            "Cost after iteration 94: 0.817702\n",
            "Accuracy = 0.50\n",
            "Cost after iteration 95: 0.818459\n",
            "Accuracy = 0.50\n",
            "Cost after iteration 96: 0.819221\n",
            "Accuracy = 0.50\n",
            "Cost after iteration 97: 0.819988\n",
            "Accuracy = 0.50\n",
            "Cost after iteration 98: 0.820760\n",
            "Accuracy = 0.51\n",
            "Cost after iteration 99: 0.821537\n",
            "Accuracy = 0.51\n",
            "Cost after iteration 100: 0.822320\n",
            "Accuracy = 0.51\n",
            "Cost after iteration 101: 0.823108\n",
            "Accuracy = 0.51\n",
            "Cost after iteration 102: 0.823902\n",
            "Accuracy = 0.51\n",
            "Cost after iteration 103: 0.824701\n",
            "Accuracy = 0.51\n",
            "Cost after iteration 104: 0.825506\n",
            "Accuracy = 0.51\n",
            "Cost after iteration 105: 0.826316\n",
            "Accuracy = 0.51\n",
            "Cost after iteration 106: 0.827132\n",
            "Accuracy = 0.51\n",
            "Cost after iteration 107: 0.827953\n",
            "Accuracy = 0.51\n",
            "Cost after iteration 108: 0.828781\n",
            "Accuracy = 0.51\n",
            "Cost after iteration 109: 0.829614\n",
            "Accuracy = 0.51\n",
            "Cost after iteration 110: 0.830452\n",
            "Accuracy = 0.51\n",
            "Cost after iteration 111: 0.831297\n",
            "Accuracy = 0.51\n",
            "Cost after iteration 112: 0.832147\n",
            "Accuracy = 0.51\n",
            "Cost after iteration 113: 0.833003\n",
            "Accuracy = 0.51\n",
            "Cost after iteration 114: 0.833864\n",
            "Accuracy = 0.51\n",
            "Cost after iteration 115: 0.834732\n",
            "Accuracy = 0.51\n",
            "Cost after iteration 116: 0.835605\n",
            "Accuracy = 0.51\n",
            "Cost after iteration 117: 0.836484\n",
            "Accuracy = 0.51\n",
            "Cost after iteration 118: 0.837368\n",
            "Accuracy = 0.51\n",
            "Cost after iteration 119: 0.838258\n",
            "Accuracy = 0.51\n",
            "Cost after iteration 120: 0.839154\n",
            "Accuracy = 0.51\n",
            "Cost after iteration 121: 0.840055\n",
            "Accuracy = 0.51\n",
            "Cost after iteration 122: 0.840962\n",
            "Accuracy = 0.51\n",
            "Cost after iteration 123: 0.841875\n",
            "Accuracy = 0.51\n",
            "Cost after iteration 124: 0.842793\n",
            "Accuracy = 0.51\n",
            "Cost after iteration 125: 0.843716\n",
            "Accuracy = 0.51\n",
            "Cost after iteration 126: 0.844645\n",
            "Accuracy = 0.51\n",
            "Cost after iteration 127: 0.845579\n",
            "Accuracy = 0.51\n",
            "Cost after iteration 128: 0.846518\n",
            "Accuracy = 0.51\n",
            "Cost after iteration 129: 0.847463\n",
            "Accuracy = 0.51\n",
            "Cost after iteration 130: 0.848412\n",
            "Accuracy = 0.51\n",
            "Cost after iteration 131: 0.849367\n",
            "Accuracy = 0.51\n",
            "Cost after iteration 132: 0.850327\n",
            "Accuracy = 0.51\n",
            "Cost after iteration 133: 0.851291\n",
            "Accuracy = 0.51\n",
            "Cost after iteration 134: 0.852261\n",
            "Accuracy = 0.51\n",
            "Cost after iteration 135: 0.853235\n",
            "Accuracy = 0.51\n",
            "Cost after iteration 136: 0.854213\n",
            "Accuracy = 0.51\n",
            "Cost after iteration 137: 0.855197\n",
            "Accuracy = 0.51\n",
            "Cost after iteration 138: 0.856184\n",
            "Accuracy = 0.51\n",
            "Cost after iteration 139: 0.857176\n",
            "Accuracy = 0.51\n",
            "Cost after iteration 140: 0.858172\n",
            "Accuracy = 0.51\n",
            "Cost after iteration 141: 0.859173\n",
            "Accuracy = 0.51\n",
            "Cost after iteration 142: 0.860177\n",
            "Accuracy = 0.52\n",
            "Cost after iteration 143: 0.861185\n",
            "Accuracy = 0.52\n",
            "Cost after iteration 144: 0.862197\n",
            "Accuracy = 0.52\n",
            "Cost after iteration 145: 0.863213\n",
            "Accuracy = 0.52\n",
            "Cost after iteration 146: 0.864232\n",
            "Accuracy = 0.52\n",
            "Cost after iteration 147: 0.865255\n",
            "Accuracy = 0.52\n",
            "Cost after iteration 148: 0.866281\n",
            "Accuracy = 0.52\n",
            "Cost after iteration 149: 0.867310\n",
            "Accuracy = 0.52\n",
            "Cost after iteration 150: 0.868342\n",
            "Accuracy = 0.52\n",
            "Cost after iteration 151: 0.869377\n",
            "Accuracy = 0.52\n",
            "Cost after iteration 152: 0.870415\n",
            "Accuracy = 0.51\n",
            "Cost after iteration 153: 0.871455\n",
            "Accuracy = 0.51\n",
            "Cost after iteration 154: 0.872497\n",
            "Accuracy = 0.51\n",
            "Cost after iteration 155: 0.873542\n",
            "Accuracy = 0.51\n",
            "Cost after iteration 156: 0.874590\n",
            "Accuracy = 0.51\n",
            "Cost after iteration 157: 0.875639\n",
            "Accuracy = 0.52\n",
            "Cost after iteration 158: 0.876690\n",
            "Accuracy = 0.52\n",
            "Cost after iteration 159: 0.877742\n",
            "Accuracy = 0.52\n",
            "Cost after iteration 160: 0.878796\n",
            "Accuracy = 0.52\n",
            "Cost after iteration 161: 0.879852\n",
            "Accuracy = 0.52\n",
            "Cost after iteration 162: 0.880908\n",
            "Accuracy = 0.52\n",
            "Cost after iteration 163: 0.881966\n",
            "Accuracy = 0.52\n",
            "Cost after iteration 164: 0.883024\n",
            "Accuracy = 0.53\n",
            "Cost after iteration 165: 0.884083\n",
            "Accuracy = 0.53\n",
            "Cost after iteration 166: 0.885143\n",
            "Accuracy = 0.53\n",
            "Cost after iteration 167: 0.886203\n",
            "Accuracy = 0.53\n",
            "Cost after iteration 168: 0.887262\n",
            "Accuracy = 0.53\n",
            "Cost after iteration 169: 0.888322\n",
            "Accuracy = 0.53\n",
            "Cost after iteration 170: 0.889382\n",
            "Accuracy = 0.53\n",
            "Cost after iteration 171: 0.890441\n",
            "Accuracy = 0.53\n",
            "Cost after iteration 172: 0.891499\n",
            "Accuracy = 0.53\n",
            "Cost after iteration 173: 0.892557\n",
            "Accuracy = 0.53\n",
            "Cost after iteration 174: 0.893613\n",
            "Accuracy = 0.53\n",
            "Cost after iteration 175: 0.894669\n",
            "Accuracy = 0.53\n",
            "Cost after iteration 176: 0.895723\n",
            "Accuracy = 0.53\n",
            "Cost after iteration 177: 0.896775\n",
            "Accuracy = 0.53\n",
            "Cost after iteration 178: 0.897826\n",
            "Accuracy = 0.53\n",
            "Cost after iteration 179: 0.898874\n",
            "Accuracy = 0.53\n",
            "Cost after iteration 180: 0.899921\n",
            "Accuracy = 0.53\n",
            "Cost after iteration 181: 0.900965\n",
            "Accuracy = 0.53\n",
            "Cost after iteration 182: 0.902007\n",
            "Accuracy = 0.53\n",
            "Cost after iteration 183: 0.903045\n",
            "Accuracy = 0.53\n",
            "Cost after iteration 184: 0.904081\n",
            "Accuracy = 0.53\n",
            "Cost after iteration 185: 0.905114\n",
            "Accuracy = 0.53\n",
            "Cost after iteration 186: 0.906143\n",
            "Accuracy = 0.53\n",
            "Cost after iteration 187: 0.907169\n",
            "Accuracy = 0.53\n",
            "Cost after iteration 188: 0.908191\n",
            "Accuracy = 0.53\n",
            "Cost after iteration 189: 0.909209\n",
            "Accuracy = 0.53\n",
            "Cost after iteration 190: 0.910223\n",
            "Accuracy = 0.53\n",
            "Cost after iteration 191: 0.911233\n",
            "Accuracy = 0.53\n",
            "Cost after iteration 192: 0.912239\n",
            "Accuracy = 0.53\n",
            "Cost after iteration 193: 0.913239\n",
            "Accuracy = 0.53\n",
            "Cost after iteration 194: 0.914235\n",
            "Accuracy = 0.53\n",
            "Cost after iteration 195: 0.915226\n",
            "Accuracy = 0.53\n",
            "Cost after iteration 196: 0.916212\n",
            "Accuracy = 0.53\n",
            "Cost after iteration 197: 0.917192\n",
            "Accuracy = 0.53\n",
            "Cost after iteration 198: 0.918167\n",
            "Accuracy = 0.53\n",
            "Cost after iteration 199: 0.919136\n",
            "Accuracy = 0.53\n",
            "Cost after iteration 200: 0.920099\n",
            "Accuracy = 0.53\n",
            "Cost after iteration 201: 0.921057\n",
            "Accuracy = 0.53\n",
            "Cost after iteration 202: 0.922008\n",
            "Accuracy = 0.53\n",
            "Cost after iteration 203: 0.922952\n",
            "Accuracy = 0.53\n",
            "Cost after iteration 204: 0.923890\n",
            "Accuracy = 0.53\n",
            "Cost after iteration 205: 0.924821\n",
            "Accuracy = 0.53\n",
            "Cost after iteration 206: 0.925745\n",
            "Accuracy = 0.53\n",
            "Cost after iteration 207: 0.926663\n",
            "Accuracy = 0.53\n",
            "Cost after iteration 208: 0.927572\n",
            "Accuracy = 0.53\n",
            "Cost after iteration 209: 0.928475\n",
            "Accuracy = 0.53\n",
            "Cost after iteration 210: 0.929369\n",
            "Accuracy = 0.53\n",
            "Cost after iteration 211: 0.930256\n",
            "Accuracy = 0.53\n",
            "Cost after iteration 212: 0.931135\n",
            "Accuracy = 0.53\n",
            "Cost after iteration 213: 0.932006\n",
            "Accuracy = 0.54\n",
            "Cost after iteration 214: 0.932868\n",
            "Accuracy = 0.54\n",
            "Cost after iteration 215: 0.933722\n",
            "Accuracy = 0.54\n",
            "Cost after iteration 216: 0.934566\n",
            "Accuracy = 0.54\n",
            "Cost after iteration 217: 0.935402\n",
            "Accuracy = 0.55\n",
            "Cost after iteration 218: 0.936229\n",
            "Accuracy = 0.55\n",
            "Cost after iteration 219: 0.937046\n",
            "Accuracy = 0.55\n",
            "Cost after iteration 220: 0.937853\n",
            "Accuracy = 0.55\n",
            "Cost after iteration 221: 0.938650\n",
            "Accuracy = 0.55\n",
            "Cost after iteration 222: 0.939437\n",
            "Accuracy = 0.55\n",
            "Cost after iteration 223: 0.940214\n",
            "Accuracy = 0.55\n",
            "Cost after iteration 224: 0.940980\n",
            "Accuracy = 0.55\n",
            "Cost after iteration 225: 0.941735\n",
            "Accuracy = 0.55\n",
            "Cost after iteration 226: 0.942478\n",
            "Accuracy = 0.55\n",
            "Cost after iteration 227: 0.943210\n",
            "Accuracy = 0.55\n",
            "Cost after iteration 228: 0.943931\n",
            "Accuracy = 0.55\n",
            "Cost after iteration 229: 0.944639\n",
            "Accuracy = 0.55\n",
            "Cost after iteration 230: 0.945334\n",
            "Accuracy = 0.55\n",
            "Cost after iteration 231: 0.946017\n",
            "Accuracy = 0.55\n",
            "Cost after iteration 232: 0.946686\n",
            "Accuracy = 0.56\n",
            "Cost after iteration 233: 0.947343\n",
            "Accuracy = 0.56\n",
            "Cost after iteration 234: 0.947985\n",
            "Accuracy = 0.56\n",
            "Cost after iteration 235: 0.948613\n",
            "Accuracy = 0.56\n",
            "Cost after iteration 236: 0.949227\n",
            "Accuracy = 0.56\n",
            "Cost after iteration 237: 0.949825\n",
            "Accuracy = 0.56\n",
            "Cost after iteration 238: 0.950409\n",
            "Accuracy = 0.56\n",
            "Cost after iteration 239: 0.950977\n",
            "Accuracy = 0.56\n",
            "Cost after iteration 240: 0.951529\n",
            "Accuracy = 0.57\n",
            "Cost after iteration 241: 0.952065\n",
            "Accuracy = 0.57\n",
            "Cost after iteration 242: 0.952585\n",
            "Accuracy = 0.57\n",
            "Cost after iteration 243: 0.953087\n",
            "Accuracy = 0.57\n",
            "Cost after iteration 244: 0.953572\n",
            "Accuracy = 0.57\n",
            "Cost after iteration 245: 0.954040\n",
            "Accuracy = 0.57\n",
            "Cost after iteration 246: 0.954489\n",
            "Accuracy = 0.57\n",
            "Cost after iteration 247: 0.954921\n",
            "Accuracy = 0.57\n",
            "Cost after iteration 248: 0.955334\n",
            "Accuracy = 0.56\n",
            "Cost after iteration 249: 0.955728\n",
            "Accuracy = 0.56\n",
            "Cost after iteration 250: 0.956104\n",
            "Accuracy = 0.56\n",
            "Cost after iteration 251: 0.956460\n",
            "Accuracy = 0.56\n",
            "Cost after iteration 252: 0.956797\n",
            "Accuracy = 0.56\n",
            "Cost after iteration 253: 0.957114\n",
            "Accuracy = 0.56\n",
            "Cost after iteration 254: 0.957411\n",
            "Accuracy = 0.58\n",
            "Cost after iteration 255: 0.957688\n",
            "Accuracy = 0.58\n",
            "Cost after iteration 256: 0.957946\n",
            "Accuracy = 0.58\n",
            "Cost after iteration 257: 0.958183\n",
            "Accuracy = 0.58\n",
            "Cost after iteration 258: 0.958400\n",
            "Accuracy = 0.58\n",
            "Cost after iteration 259: 0.958597\n",
            "Accuracy = 0.58\n",
            "Cost after iteration 260: 0.958773\n",
            "Accuracy = 0.58\n",
            "Cost after iteration 261: 0.958929\n",
            "Accuracy = 0.58\n",
            "Cost after iteration 262: 0.959065\n",
            "Accuracy = 0.58\n",
            "Cost after iteration 263: 0.959181\n",
            "Accuracy = 0.58\n",
            "Cost after iteration 264: 0.959277\n",
            "Accuracy = 0.58\n",
            "Cost after iteration 265: 0.959353\n",
            "Accuracy = 0.58\n",
            "Cost after iteration 266: 0.959408\n",
            "Accuracy = 0.58\n",
            "Cost after iteration 267: 0.959445\n",
            "Accuracy = 0.58\n",
            "Cost after iteration 268: 0.959461\n",
            "Accuracy = 0.58\n",
            "Cost after iteration 269: 0.959459\n",
            "Accuracy = 0.58\n",
            "Cost after iteration 270: 0.959437\n",
            "Accuracy = 0.58\n",
            "Cost after iteration 271: 0.959397\n",
            "Accuracy = 0.58\n",
            "Cost after iteration 272: 0.959338\n",
            "Accuracy = 0.58\n",
            "Cost after iteration 273: 0.959260\n",
            "Accuracy = 0.58\n",
            "Cost after iteration 274: 0.959165\n",
            "Accuracy = 0.58\n",
            "Cost after iteration 275: 0.959052\n",
            "Accuracy = 0.58\n",
            "Cost after iteration 276: 0.958922\n",
            "Accuracy = 0.58\n",
            "Cost after iteration 277: 0.958774\n",
            "Accuracy = 0.59\n",
            "Cost after iteration 278: 0.958610\n",
            "Accuracy = 0.58\n",
            "Cost after iteration 279: 0.958430\n",
            "Accuracy = 0.58\n",
            "Cost after iteration 280: 0.958234\n",
            "Accuracy = 0.58\n",
            "Cost after iteration 281: 0.958022\n",
            "Accuracy = 0.59\n",
            "Cost after iteration 282: 0.957794\n",
            "Accuracy = 0.59\n",
            "Cost after iteration 283: 0.957552\n",
            "Accuracy = 0.59\n",
            "Cost after iteration 284: 0.957295\n",
            "Accuracy = 0.59\n",
            "Cost after iteration 285: 0.957024\n",
            "Accuracy = 0.59\n",
            "Cost after iteration 286: 0.956740\n",
            "Accuracy = 0.60\n",
            "Cost after iteration 287: 0.956442\n",
            "Accuracy = 0.60\n",
            "Cost after iteration 288: 0.956130\n",
            "Accuracy = 0.60\n",
            "Cost after iteration 289: 0.955807\n",
            "Accuracy = 0.60\n",
            "Cost after iteration 290: 0.955470\n",
            "Accuracy = 0.60\n",
            "Cost after iteration 291: 0.955122\n",
            "Accuracy = 0.60\n",
            "Cost after iteration 292: 0.954762\n",
            "Accuracy = 0.60\n",
            "Cost after iteration 293: 0.954391\n",
            "Accuracy = 0.60\n",
            "Cost after iteration 294: 0.954009\n",
            "Accuracy = 0.60\n",
            "Cost after iteration 295: 0.953617\n",
            "Accuracy = 0.60\n",
            "Cost after iteration 296: 0.953214\n",
            "Accuracy = 0.60\n",
            "Cost after iteration 297: 0.952801\n",
            "Accuracy = 0.60\n",
            "Cost after iteration 298: 0.952379\n",
            "Accuracy = 0.60\n",
            "Cost after iteration 299: 0.951947\n",
            "Accuracy = 0.60\n",
            "Cost after iteration 300: 0.951506\n",
            "Accuracy = 0.61\n",
            "Cost after iteration 301: 0.951057\n",
            "Accuracy = 0.60\n",
            "Cost after iteration 302: 0.950599\n",
            "Accuracy = 0.61\n",
            "Cost after iteration 303: 0.950133\n",
            "Accuracy = 0.61\n",
            "Cost after iteration 304: 0.949659\n",
            "Accuracy = 0.61\n",
            "Cost after iteration 305: 0.949178\n",
            "Accuracy = 0.61\n",
            "Cost after iteration 306: 0.948689\n",
            "Accuracy = 0.61\n",
            "Cost after iteration 307: 0.948193\n",
            "Accuracy = 0.61\n",
            "Cost after iteration 308: 0.947691\n",
            "Accuracy = 0.61\n",
            "Cost after iteration 309: 0.947182\n",
            "Accuracy = 0.61\n",
            "Cost after iteration 310: 0.946667\n",
            "Accuracy = 0.61\n",
            "Cost after iteration 311: 0.946145\n",
            "Accuracy = 0.61\n",
            "Cost after iteration 312: 0.945618\n",
            "Accuracy = 0.61\n",
            "Cost after iteration 313: 0.945086\n",
            "Accuracy = 0.61\n",
            "Cost after iteration 314: 0.944548\n",
            "Accuracy = 0.61\n",
            "Cost after iteration 315: 0.944005\n",
            "Accuracy = 0.61\n",
            "Cost after iteration 316: 0.943457\n",
            "Accuracy = 0.61\n",
            "Cost after iteration 317: 0.942905\n",
            "Accuracy = 0.61\n",
            "Cost after iteration 318: 0.942348\n",
            "Accuracy = 0.61\n",
            "Cost after iteration 319: 0.941788\n",
            "Accuracy = 0.61\n",
            "Cost after iteration 320: 0.941223\n",
            "Accuracy = 0.61\n",
            "Cost after iteration 321: 0.940654\n",
            "Accuracy = 0.61\n",
            "Cost after iteration 322: 0.940082\n",
            "Accuracy = 0.61\n",
            "Cost after iteration 323: 0.939507\n",
            "Accuracy = 0.61\n",
            "Cost after iteration 324: 0.938928\n",
            "Accuracy = 0.61\n",
            "Cost after iteration 325: 0.938347\n",
            "Accuracy = 0.61\n",
            "Cost after iteration 326: 0.937762\n",
            "Accuracy = 0.61\n",
            "Cost after iteration 327: 0.937176\n",
            "Accuracy = 0.61\n",
            "Cost after iteration 328: 0.936587\n",
            "Accuracy = 0.61\n",
            "Cost after iteration 329: 0.935996\n",
            "Accuracy = 0.61\n",
            "Cost after iteration 330: 0.935403\n",
            "Accuracy = 0.61\n",
            "Cost after iteration 331: 0.934808\n",
            "Accuracy = 0.61\n",
            "Cost after iteration 332: 0.934212\n",
            "Accuracy = 0.61\n",
            "Cost after iteration 333: 0.933614\n",
            "Accuracy = 0.61\n",
            "Cost after iteration 334: 0.933015\n",
            "Accuracy = 0.61\n",
            "Cost after iteration 335: 0.932415\n",
            "Accuracy = 0.61\n",
            "Cost after iteration 336: 0.931814\n",
            "Accuracy = 0.61\n",
            "Cost after iteration 337: 0.931213\n",
            "Accuracy = 0.61\n",
            "Cost after iteration 338: 0.930611\n",
            "Accuracy = 0.61\n",
            "Cost after iteration 339: 0.930008\n",
            "Accuracy = 0.62\n",
            "Cost after iteration 340: 0.929406\n",
            "Accuracy = 0.62\n",
            "Cost after iteration 341: 0.928803\n",
            "Accuracy = 0.62\n",
            "Cost after iteration 342: 0.928201\n",
            "Accuracy = 0.62\n",
            "Cost after iteration 343: 0.927598\n",
            "Accuracy = 0.62\n",
            "Cost after iteration 344: 0.926996\n",
            "Accuracy = 0.63\n",
            "Cost after iteration 345: 0.926395\n",
            "Accuracy = 0.63\n",
            "Cost after iteration 346: 0.925794\n",
            "Accuracy = 0.63\n",
            "Cost after iteration 347: 0.925194\n",
            "Accuracy = 0.63\n",
            "Cost after iteration 348: 0.924595\n",
            "Accuracy = 0.63\n",
            "Cost after iteration 349: 0.923997\n",
            "Accuracy = 0.63\n",
            "Cost after iteration 350: 0.923400\n",
            "Accuracy = 0.63\n",
            "Cost after iteration 351: 0.922804\n",
            "Accuracy = 0.63\n",
            "Cost after iteration 352: 0.922210\n",
            "Accuracy = 0.63\n",
            "Cost after iteration 353: 0.921616\n",
            "Accuracy = 0.63\n",
            "Cost after iteration 354: 0.921025\n",
            "Accuracy = 0.63\n",
            "Cost after iteration 355: 0.920434\n",
            "Accuracy = 0.63\n",
            "Cost after iteration 356: 0.919846\n",
            "Accuracy = 0.63\n",
            "Cost after iteration 357: 0.919259\n",
            "Accuracy = 0.63\n",
            "Cost after iteration 358: 0.918673\n",
            "Accuracy = 0.63\n",
            "Cost after iteration 359: 0.918090\n",
            "Accuracy = 0.63\n",
            "Cost after iteration 360: 0.917508\n",
            "Accuracy = 0.63\n",
            "Cost after iteration 361: 0.916928\n",
            "Accuracy = 0.63\n",
            "Cost after iteration 362: 0.916350\n",
            "Accuracy = 0.63\n",
            "Cost after iteration 363: 0.915774\n",
            "Accuracy = 0.63\n",
            "Cost after iteration 364: 0.915200\n",
            "Accuracy = 0.63\n",
            "Cost after iteration 365: 0.914628\n",
            "Accuracy = 0.63\n",
            "Cost after iteration 366: 0.914058\n",
            "Accuracy = 0.63\n",
            "Cost after iteration 367: 0.913490\n",
            "Accuracy = 0.63\n",
            "Cost after iteration 368: 0.912923\n",
            "Accuracy = 0.63\n",
            "Cost after iteration 369: 0.912359\n",
            "Accuracy = 0.63\n",
            "Cost after iteration 370: 0.911797\n",
            "Accuracy = 0.63\n",
            "Cost after iteration 371: 0.911237\n",
            "Accuracy = 0.64\n",
            "Cost after iteration 372: 0.910678\n",
            "Accuracy = 0.64\n",
            "Cost after iteration 373: 0.910122\n",
            "Accuracy = 0.64\n",
            "Cost after iteration 374: 0.909567\n",
            "Accuracy = 0.64\n",
            "Cost after iteration 375: 0.909014\n",
            "Accuracy = 0.64\n",
            "Cost after iteration 376: 0.908464\n",
            "Accuracy = 0.64\n",
            "Cost after iteration 377: 0.907914\n",
            "Accuracy = 0.64\n",
            "Cost after iteration 378: 0.907367\n",
            "Accuracy = 0.64\n",
            "Cost after iteration 379: 0.906822\n",
            "Accuracy = 0.64\n",
            "Cost after iteration 380: 0.906278\n",
            "Accuracy = 0.64\n",
            "Cost after iteration 381: 0.905736\n",
            "Accuracy = 0.64\n",
            "Cost after iteration 382: 0.905195\n",
            "Accuracy = 0.64\n",
            "Cost after iteration 383: 0.904657\n",
            "Accuracy = 0.64\n",
            "Cost after iteration 384: 0.904119\n",
            "Accuracy = 0.64\n",
            "Cost after iteration 385: 0.903583\n",
            "Accuracy = 0.64\n",
            "Cost after iteration 386: 0.903049\n",
            "Accuracy = 0.64\n",
            "Cost after iteration 387: 0.902516\n",
            "Accuracy = 0.64\n",
            "Cost after iteration 388: 0.901985\n",
            "Accuracy = 0.64\n",
            "Cost after iteration 389: 0.901454\n",
            "Accuracy = 0.64\n",
            "Cost after iteration 390: 0.900925\n",
            "Accuracy = 0.64\n",
            "Cost after iteration 391: 0.900397\n",
            "Accuracy = 0.64\n",
            "Cost after iteration 392: 0.899871\n",
            "Accuracy = 0.64\n",
            "Cost after iteration 393: 0.899345\n",
            "Accuracy = 0.64\n",
            "Cost after iteration 394: 0.898821\n",
            "Accuracy = 0.64\n",
            "Cost after iteration 395: 0.898297\n",
            "Accuracy = 0.64\n",
            "Cost after iteration 396: 0.897774\n",
            "Accuracy = 0.64\n",
            "Cost after iteration 397: 0.897253\n",
            "Accuracy = 0.64\n",
            "Cost after iteration 398: 0.896732\n",
            "Accuracy = 0.64\n",
            "Cost after iteration 399: 0.896212\n",
            "Accuracy = 0.64\n",
            "Cost after iteration 400: 0.895693\n",
            "Accuracy = 0.64\n",
            "Cost after iteration 401: 0.895174\n",
            "Accuracy = 0.64\n",
            "Cost after iteration 402: 0.894656\n",
            "Accuracy = 0.65\n",
            "Cost after iteration 403: 0.894139\n",
            "Accuracy = 0.65\n",
            "Cost after iteration 404: 0.893622\n",
            "Accuracy = 0.65\n",
            "Cost after iteration 405: 0.893106\n",
            "Accuracy = 0.65\n",
            "Cost after iteration 406: 0.892591\n",
            "Accuracy = 0.65\n",
            "Cost after iteration 407: 0.892075\n",
            "Accuracy = 0.65\n",
            "Cost after iteration 408: 0.891560\n",
            "Accuracy = 0.65\n",
            "Cost after iteration 409: 0.891046\n",
            "Accuracy = 0.64\n",
            "Cost after iteration 410: 0.890532\n",
            "Accuracy = 0.65\n",
            "Cost after iteration 411: 0.890018\n",
            "Accuracy = 0.64\n",
            "Cost after iteration 412: 0.889504\n",
            "Accuracy = 0.65\n",
            "Cost after iteration 413: 0.888991\n",
            "Accuracy = 0.65\n",
            "Cost after iteration 414: 0.888478\n",
            "Accuracy = 0.65\n",
            "Cost after iteration 415: 0.887965\n",
            "Accuracy = 0.65\n",
            "Cost after iteration 416: 0.887452\n",
            "Accuracy = 0.65\n",
            "Cost after iteration 417: 0.886939\n",
            "Accuracy = 0.65\n",
            "Cost after iteration 418: 0.886426\n",
            "Accuracy = 0.65\n",
            "Cost after iteration 419: 0.885913\n",
            "Accuracy = 0.65\n",
            "Cost after iteration 420: 0.885400\n",
            "Accuracy = 0.65\n",
            "Cost after iteration 421: 0.884888\n",
            "Accuracy = 0.65\n",
            "Cost after iteration 422: 0.884375\n",
            "Accuracy = 0.65\n",
            "Cost after iteration 423: 0.883862\n",
            "Accuracy = 0.65\n",
            "Cost after iteration 424: 0.883349\n",
            "Accuracy = 0.66\n",
            "Cost after iteration 425: 0.882835\n",
            "Accuracy = 0.65\n",
            "Cost after iteration 426: 0.882322\n",
            "Accuracy = 0.65\n",
            "Cost after iteration 427: 0.881808\n",
            "Accuracy = 0.66\n",
            "Cost after iteration 428: 0.881295\n",
            "Accuracy = 0.66\n",
            "Cost after iteration 429: 0.880781\n",
            "Accuracy = 0.66\n",
            "Cost after iteration 430: 0.880266\n",
            "Accuracy = 0.66\n",
            "Cost after iteration 431: 0.879752\n",
            "Accuracy = 0.66\n",
            "Cost after iteration 432: 0.879237\n",
            "Accuracy = 0.66\n",
            "Cost after iteration 433: 0.878722\n",
            "Accuracy = 0.66\n",
            "Cost after iteration 434: 0.878207\n",
            "Accuracy = 0.66\n",
            "Cost after iteration 435: 0.877691\n",
            "Accuracy = 0.66\n",
            "Cost after iteration 436: 0.877175\n",
            "Accuracy = 0.66\n",
            "Cost after iteration 437: 0.876658\n",
            "Accuracy = 0.66\n",
            "Cost after iteration 438: 0.876142\n",
            "Accuracy = 0.66\n",
            "Cost after iteration 439: 0.875624\n",
            "Accuracy = 0.66\n",
            "Cost after iteration 440: 0.875107\n",
            "Accuracy = 0.66\n",
            "Cost after iteration 441: 0.874589\n",
            "Accuracy = 0.66\n",
            "Cost after iteration 442: 0.874070\n",
            "Accuracy = 0.66\n",
            "Cost after iteration 443: 0.873552\n",
            "Accuracy = 0.66\n",
            "Cost after iteration 444: 0.873032\n",
            "Accuracy = 0.66\n",
            "Cost after iteration 445: 0.872513\n",
            "Accuracy = 0.66\n",
            "Cost after iteration 446: 0.871993\n",
            "Accuracy = 0.66\n",
            "Cost after iteration 447: 0.871472\n",
            "Accuracy = 0.66\n",
            "Cost after iteration 448: 0.870951\n",
            "Accuracy = 0.66\n",
            "Cost after iteration 449: 0.870429\n",
            "Accuracy = 0.66\n",
            "Cost after iteration 450: 0.869907\n",
            "Accuracy = 0.66\n",
            "Cost after iteration 451: 0.869385\n",
            "Accuracy = 0.66\n",
            "Cost after iteration 452: 0.868862\n",
            "Accuracy = 0.66\n",
            "Cost after iteration 453: 0.868338\n",
            "Accuracy = 0.66\n",
            "Cost after iteration 454: 0.867814\n",
            "Accuracy = 0.66\n",
            "Cost after iteration 455: 0.867289\n",
            "Accuracy = 0.66\n",
            "Cost after iteration 456: 0.866764\n",
            "Accuracy = 0.66\n",
            "Cost after iteration 457: 0.866239\n",
            "Accuracy = 0.66\n",
            "Cost after iteration 458: 0.865712\n",
            "Accuracy = 0.66\n",
            "Cost after iteration 459: 0.865186\n",
            "Accuracy = 0.66\n",
            "Cost after iteration 460: 0.864658\n",
            "Accuracy = 0.66\n",
            "Cost after iteration 461: 0.864130\n",
            "Accuracy = 0.66\n",
            "Cost after iteration 462: 0.863602\n",
            "Accuracy = 0.66\n",
            "Cost after iteration 463: 0.863073\n",
            "Accuracy = 0.66\n",
            "Cost after iteration 464: 0.862543\n",
            "Accuracy = 0.66\n",
            "Cost after iteration 465: 0.862013\n",
            "Accuracy = 0.66\n",
            "Cost after iteration 466: 0.861482\n",
            "Accuracy = 0.66\n",
            "Cost after iteration 467: 0.860951\n",
            "Accuracy = 0.66\n",
            "Cost after iteration 468: 0.860419\n",
            "Accuracy = 0.65\n",
            "Cost after iteration 469: 0.859887\n",
            "Accuracy = 0.65\n",
            "Cost after iteration 470: 0.859354\n",
            "Accuracy = 0.65\n",
            "Cost after iteration 471: 0.858820\n",
            "Accuracy = 0.65\n",
            "Cost after iteration 472: 0.858286\n",
            "Accuracy = 0.65\n",
            "Cost after iteration 473: 0.857751\n",
            "Accuracy = 0.65\n",
            "Cost after iteration 474: 0.857215\n",
            "Accuracy = 0.65\n",
            "Cost after iteration 475: 0.856679\n",
            "Accuracy = 0.65\n",
            "Cost after iteration 476: 0.856143\n",
            "Accuracy = 0.65\n",
            "Cost after iteration 477: 0.855605\n",
            "Accuracy = 0.65\n",
            "Cost after iteration 478: 0.855067\n",
            "Accuracy = 0.65\n",
            "Cost after iteration 479: 0.854529\n",
            "Accuracy = 0.65\n",
            "Cost after iteration 480: 0.853990\n",
            "Accuracy = 0.65\n",
            "Cost after iteration 481: 0.853450\n",
            "Accuracy = 0.65\n",
            "Cost after iteration 482: 0.852910\n",
            "Accuracy = 0.65\n",
            "Cost after iteration 483: 0.852369\n",
            "Accuracy = 0.65\n",
            "Cost after iteration 484: 0.851827\n",
            "Accuracy = 0.65\n",
            "Cost after iteration 485: 0.851285\n",
            "Accuracy = 0.65\n",
            "Cost after iteration 486: 0.850742\n",
            "Accuracy = 0.65\n",
            "Cost after iteration 487: 0.850199\n",
            "Accuracy = 0.65\n",
            "Cost after iteration 488: 0.849655\n",
            "Accuracy = 0.65\n",
            "Cost after iteration 489: 0.849110\n",
            "Accuracy = 0.65\n",
            "Cost after iteration 490: 0.848565\n",
            "Accuracy = 0.65\n",
            "Cost after iteration 491: 0.848019\n",
            "Accuracy = 0.65\n",
            "Cost after iteration 492: 0.847472\n",
            "Accuracy = 0.65\n",
            "Cost after iteration 493: 0.846925\n",
            "Accuracy = 0.65\n",
            "Cost after iteration 494: 0.846377\n",
            "Accuracy = 0.65\n",
            "Cost after iteration 495: 0.845829\n",
            "Accuracy = 0.65\n",
            "Cost after iteration 496: 0.845280\n",
            "Accuracy = 0.65\n",
            "Cost after iteration 497: 0.844730\n",
            "Accuracy = 0.65\n",
            "Cost after iteration 498: 0.844180\n",
            "Accuracy = 0.65\n",
            "Cost after iteration 499: 0.843629\n",
            "Accuracy = 0.65\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vQTVoy63jGr"
      },
      "source": [
        ""
      ],
      "execution_count": 323,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdwGChrY6pPt"
      },
      "source": [
        ""
      ],
      "execution_count": 312,
      "outputs": []
    }
  ]
}