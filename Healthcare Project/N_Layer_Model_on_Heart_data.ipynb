{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "N_Layer_Model_on_Heart_data.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNRZcHjQ6bb0Ju+m68wO2Aw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rashi2011/Rashi-Madhukar/blob/master/Healthcare%20Project/N_Layer_Model_on_Heart_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFAAgga7SzMr"
      },
      "source": [
        "#import libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHSZoZuabkFm",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "340f766a-88e2-4bda-fd5d-6c8d2174fdf4"
      },
      "source": [
        "#Loading the data()\n",
        "#Downloading the dataset from local drive\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3655732f-ddd7-47c5-8e50-e88f3f7ddbbd\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3655732f-ddd7-47c5-8e50-e88f3f7ddbbd\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving Heart.csv to Heart.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4zxnju4dm3z"
      },
      "source": [
        "import io\n",
        "dataset = pd.read_csv(io.BytesIO(uploaded['Heart.csv']))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jk-ww3NDdfTc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bd83ba8-fda4-4d16-8100-30bc5e99b175"
      },
      "source": [
        "dataset = dataset.dropna()\n",
        "X = dataset.iloc[:,1:14].values\n",
        "Y = dataset.iloc[:,14].values\n",
        "\n",
        "#Encoding Categorical Data\n",
        "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "lb = LabelEncoder()\n",
        "X[:,2] = lb.fit_transform(X[:,2])       #chest pain(typical,asymptomatic,non_typical,non-anginal)\n",
        "f=X[:,12].tolist()\n",
        "\n",
        "for i in range(len(f)):\n",
        "    if f[i]=='normal':\n",
        "        f[i]=3\n",
        "    if f[i]=='fixed':\n",
        "        f[i]=6\n",
        "    if f[i]=='reversable':\n",
        "        f[i]=7\n",
        "\n",
        "X[:,12]=f\n",
        "#use one hot encoder for chest pain column\n",
        "ct = ColumnTransformer([(\"Chest Pain\", OneHotEncoder(), [2])], remainder = 'passthrough')\n",
        "X = ct.fit_transform(X)\n",
        "X = X[:,1:]\n",
        "\n",
        "\n",
        "#imputer(It takes 2d array)\n",
        "from sklearn.impute import SimpleImputer\n",
        "imp = SimpleImputer(missing_values = np.nan, strategy = 'mean')\n",
        "X = imp.fit_transform(X)\n",
        "Y = lb.fit_transform(Y)\n",
        "\n",
        "#Plotting the distribution of class labels\n",
        "count_p=0;count_n=0\n",
        "for i in Y:\n",
        "    if i==0:\n",
        "        count_n+=1\n",
        "    else:\n",
        "        count_p+=1\n",
        "print(\"No: of positive Labels : \", count_p,\" and No: of -ve labels : \",count_n)\n",
        "\n",
        "#OneHotEncoder\n",
        "'''\n",
        "ohe = OneHotEncoder(categorical_features = [3,13])\n",
        "X = ohe.fit_transform(X).toarray()\n",
        "'''\n",
        "\n",
        "#Feature Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X = sc.fit_transform(X)\n",
        "print(X.shape)\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No: of positive Labels :  137  and No: of -ve labels :  160\n",
            "(297, 15)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjQyquGksVCQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09e8d686-704c-4498-e6c0-04d7d1c9799a"
      },
      "source": [
        "#Splitting train and test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,Y_train,Y_test = train_test_split(X,Y, test_size = 0.3,random_state = 0)\n",
        "\n",
        "#Reshaping X and Y(I/p and O/p)\n",
        "X_train = np.transpose(X_train)\n",
        "X_test = np.transpose(X_test)\n",
        "Y_train= np.expand_dims(Y_train, axis=0)\n",
        "Y_test= np.expand_dims(Y_test, axis=0)\n",
        "print(X_train.shape)\n",
        "print(Y_train.shape)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(15, 207)\n",
            "(1, 207)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfPJRsctUY1b"
      },
      "source": [
        "#Initialize Parameters\n",
        "def initialize_param(layers_dims):\n",
        "  parameters = {}                #empty dictionary\n",
        "  L = len(layers_dims)           # no of layers in the network\n",
        "  for l in range(1,L):\n",
        "    parameters[\"W\" + str(l)]= np.round(np.random.rand(layers_dims[l],layers_dims[l-1]),4)\n",
        "    parameters[\"b\"+ str(l)]= np.round(np.zeros((layers_dims[l],1)),3)\n",
        "\n",
        "  return parameters"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfnSLXT6W8pe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "4cfcf834-01a5-4368-b743-a125967eec63"
      },
      "source": [
        "'''\n",
        "layers_dims = [14,5,3,1]\n",
        "parameters = initialize_param(layers_dims)\n",
        "#print(parameters)             #Dictionary\n",
        "print(\"W1 = \"+ str(parameters[\"W1\"]))\n",
        "print(\"b1 = \"+ str(parameters[\"b1\"]))\n",
        "print(\"W2 = \"+ str(parameters[\"W2\"]))\n",
        "print(\"b2 = \"+ str(parameters[\"b2\"]))\n",
        "'''"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nlayers_dims = [14,5,3,1]\\nparameters = initialize_param(layers_dims)\\n#print(parameters)             #Dictionary\\nprint(\"W1 = \"+ str(parameters[\"W1\"]))\\nprint(\"b1 = \"+ str(parameters[\"b1\"]))\\nprint(\"W2 = \"+ str(parameters[\"W2\"]))\\nprint(\"b2 = \"+ str(parameters[\"b2\"]))\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mw6imtptkA6H"
      },
      "source": [
        "#Activation Function\n",
        "def sigmoid(z):\n",
        "  A = 1/(1+np.exp(-z))\n",
        "  return A\n",
        "\n",
        "def relu(z):\n",
        "  A = np.maximum(0,z)\n",
        "  return A\n",
        "\n",
        "def sign_f(AL,mean):\n",
        "    if AL <= mean:\n",
        "      return 0\n",
        "    else:\n",
        "      return 1\n"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRNuy4C8hcxk"
      },
      "source": [
        "#forward\n",
        "def forward(A, W, b):\n",
        "  Z = np.dot(W,A) +b\n",
        "  #print(\"Z.shape = \",Z.shape)       \n",
        "  \n",
        "  return Z"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WS8PxnydPq2"
      },
      "source": [
        "#Activations forward\n",
        "def activation_forward(A_prev, W, b ,activation):\n",
        "  if activation == \"sigmoid\":\n",
        "    #print(\"W,b\",W.shape,b.shape)\n",
        "    Z = forward(A_prev,W,b)\n",
        "    A  = sigmoid(Z)\n",
        "    \n",
        "\n",
        "  elif activation == \"relu\":\n",
        "    #print(\"W,b\",W.shape,b.shape)\n",
        "    Z = forward(A_prev,W,b)\n",
        "    A  =  relu(Z)\n",
        "    #print(\"Ashapes\",A.shape)\n",
        "          \n",
        "  return A,Z"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqKWF8huYalD"
      },
      "source": [
        "#Forward Propagation\n",
        "def forward_model(X,parameters):    \n",
        "  cache = {}  #empty dictionary    \n",
        "  A = X\n",
        "  L = len(parameters)//2\n",
        " \n",
        "  for l in range(1,L):\n",
        "    A_prev =A\n",
        "    A,cache[\"Z\"+str(l)] = activation_forward(A_prev, parameters['W'+ str(l)], parameters['b'+ str(l)], activation = \"relu\")  #i = 1,i = 2\n",
        "    cache[\"A\"+str(l)] = A\n",
        "    \n",
        "    \n",
        "  AL,cache[\"Z\"+str(L)] = activation_forward(cache[\"A\"+str(L-1)], parameters[\"W\"+str(L)], parameters[\"b\"+str(L)], activation = \"sigmoid\")  #i = 3\n",
        "  cache[\"A\"+str(L)] = AL\n",
        "  return AL,cache\n",
        "\n"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGN_q39a6BFV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "67915ba9-7a84-4c4f-a21f-55cb38cc5dc8"
      },
      "source": [
        "#tocheck'''\n",
        "'''\n",
        "AL,cache = forward_model(X_train,parameters)\n",
        "print(AL.shape)\n",
        "'''"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nAL,cache = forward_model(X_train,parameters)\\nprint(AL.shape)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnCP10rOsYAg"
      },
      "source": [
        "#Cost Function\n",
        "def compute_cost(AL,Y):\n",
        "  m = Y.shape[1]\n",
        "  #print(m)\n",
        "  cost = -(1/m)*np.sum((Y*np.log(AL) + (1-Y)*np.log(1-AL)))\n",
        "  return cost"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEajzLx3Ih0-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c3e3cd65-aa25-4fd2-d05e-0fe6ace923da"
      },
      "source": [
        "#to check\n",
        "'''\n",
        "cost = compute_cost(AL,Y_train)\n",
        "print(\"cost = \",cost)\n",
        "'''"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ncost = compute_cost(AL,Y_train)\\nprint(\"cost = \",cost)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cx4BS-9QlPJM"
      },
      "source": [
        "\n",
        "def sigmoid_backward(dA,AL):\n",
        "  \n",
        "  dZ = np.multiply(AL,(1-AL))\n",
        "  Loss = np.multiply(dA,AL)             #(1x207)\n",
        "  return Loss\n",
        "\n",
        "\n",
        "def relu_backward(dA,AL):\n",
        "  dZ= np.zeros((AL.shape))\n",
        "  for i in range(AL.shape[0]):\n",
        "    for j in range(AL.shape[1]):\n",
        "        \n",
        "        if AL[i][j] <= 0:\n",
        "          dZ[i][j] = dZ[i][j]+ 0\n",
        "        else:\n",
        "          dZ[i][j] =dZ[i][j] + 1\n",
        "  #print(dZ.shape)               #(5x207)\n",
        "  dZ = np.multiply(dZ,AL)\n",
        "  Loss = np.multiply(dA,dZ)\n",
        "\n",
        "  return Loss"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsgYOa8FtGuo"
      },
      "source": [
        "#Backward \n",
        "def backward(dA,W,A_prev,Loss): \n",
        "  m = dA.shape[1]            \n",
        "  dW = (1/m)*np.dot(Loss,A_prev.T)   #(1x3)\n",
        "  dA_ = np.dot(W.T,dA)       #3x207\n",
        "  #print(\"dA_\",dA_.shape)\n",
        "  db = (1/m)*np.sum(Loss,axis = 1,keepdims = True) \n",
        "\n",
        "  return dA_,dW,db"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVQTuiWLtQsp"
      },
      "source": [
        "#Activation backward\n",
        "def activation_backward(dA,W,A_prev,AL ,activation):\n",
        "  \n",
        "  if activation == \"relu\":\n",
        "    Loss = relu_backward(dA, AL )\n",
        "    dA_,dW ,db = backward(dA,W,A_prev,Loss)\n",
        "  elif activation == \"sigmoid\":\n",
        "    Loss = sigmoid_backward(dA,AL)\n",
        "    dA_,dW ,db = backward(dA,W,A_prev,Loss)\n",
        "\n",
        "  return dA_,dW,db\n",
        "\n"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJAPFL6jtVbj"
      },
      "source": [
        "#backward Model\n",
        "def backward_model(AL,X,Y, cache,parameters):\n",
        "  \n",
        "  grads = {}           #empty dictionary\n",
        "  L = int(len(parameters)/2)              #3    #no of layers  \n",
        "  #print(L)  #no of layers\n",
        "  m = Y.shape[1]                #no of training examples\n",
        "  Y = Y.reshape(AL.shape)       #to ensure shape of Y = AL\n",
        "  cache[\"A\" + str(0)] = X_train\n",
        "  #intialization of backward propagation\n",
        "  dAL = AL- Y           #dA3\n",
        "  #print(\"dAL\",dAL.shape)\n",
        "  #Sigmoid (for layer L = L-1 )       #L = 3\n",
        "  \n",
        "  grads[\"dA\"+ str(L-1)],grads[\"dW\"+ str(L)] ,grads[\"db\"+ str(L)]  = activation_backward(dAL,parameters[\"W\"+str(L)],cache[\"A\" + str(L-1)],cache[\"A\"+ str(L)] ,activation = \"sigmoid\")\n",
        "\n",
        "  # for layer (L =L-2 to  L=0 ) \n",
        "  for l in reversed(range(L-1)):    #L-1 = 2, #(0 to 2) # means l = 0,1 # reversing it  l = 1,0\n",
        "    if l!=0:\n",
        "      grads[\"dA\"+ str(l)],grads[\"dW\"+ str(l+1)] ,grads[\"db\"+ str(l+1)]  = activation_backward(grads[\"dA\"+ str(l+1)], parameters[\"W\"+ str(l+1)],cache[\"A\"+str(l)],cache[\"A\"+str(l+1)] ,activation = \"relu\")\n",
        "\n",
        "    else:\n",
        "      _,grads[\"dW\"+ str(l+1)] ,grads[\"db\"+ str(l+1)]  = activation_backward(grads[\"dA\"+ str(l+1)], parameters[\"W\"+ str(l+1)],cache[\"A\"+ str(l)] ,cache[\"A\"+str(l+1)],activation = \"relu\")\n",
        "  return grads\n",
        "     \n",
        "\n",
        "\n"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsdwcJLbytIC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6f6aac62-0e13-45ab-ecfd-4d31bf8a1a73"
      },
      "source": [
        "#To check\n",
        "'''\n",
        "grads = backward_model(AL,X_train,Y_train,cache,parameters)\n",
        "#print(grads)\n",
        "'''"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ngrads = backward_model(AL,X_train,Y_train,cache,parameters)\\n#print(grads)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8e-JlRW-SJzU"
      },
      "source": [
        "#Update Parameters\n",
        "def update_param(parameters,grads,learning_rate):\n",
        "  L = len(parameters)//2\n",
        "  for l in range(L):\n",
        "    parameters[\"W\"+str(l+1)] = parameters[\"W\"+str(l+1)] - (learning_rate*(grads[\"dW\"+ str(l+1)]))\n",
        "    parameters[\"b\"+str(l+1)] = parameters[\"b\"+str(l+1)] - (learning_rate*(grads[\"db\"+ str(l+1)]))\n",
        "\n",
        "  return parameters"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdvJDzfeIHIN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "72923163-2711-4fff-bcd2-0e490c4b8aa1"
      },
      "source": [
        "\n",
        "#To Check\n",
        "'''\n",
        "parameters = update_param(parameters,grads,learning_rate = 0.01)\n",
        "\n",
        "print(parameters[\"W\"+str(1)])\n",
        "print(parameters[\"b\"+str(1)])\n",
        "'''"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nparameters = update_param(parameters,grads,learning_rate = 0.01)\\n\\nprint(parameters[\"W\"+str(1)])\\nprint(parameters[\"b\"+str(1)])\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpf9mWZ2jgVx"
      },
      "source": [
        "from sklearn.metrics import precision_score\n",
        "#NN Model\n",
        "def L_layer_model(X, Y, layers_dims, learning_rate , num_iterations):\n",
        "  costs = []                #empty list\n",
        "  acc = []\n",
        "  \n",
        "  parameters = initialize_param(layers_dims)\n",
        "  for i in range(0,num_iterations):\n",
        "    #Forward Propagation(relu to sigmoid)\n",
        "    \n",
        "    AL,cache = forward_model(X,parameters)\n",
        "    \n",
        "\n",
        "    #compute cost\n",
        "    cost = compute_cost(AL,Y)\n",
        "\n",
        "    #Backward Propagation\n",
        "    grads = backward_model(AL,X,Y, cache,parameters)\n",
        "\n",
        "    #Update Parameters\n",
        "    parameters = update_param(parameters,grads,learning_rate)\n",
        "    \n",
        "\n",
        "    #Print the cost on every 100th training examples\n",
        "    \n",
        "    print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "    \n",
        "    #Converting to y_pred\n",
        "    \n",
        "    y_pred = []\n",
        "    for j in range(AL.shape[1]):\n",
        "      y_pred.append(sign_f(AL[0][j],np.mean(AL)))\n",
        "    y_pred = np.asmatrix(y_pred)\n",
        "    \n",
        "    costs.append(cost)\n",
        "    #Accuracy\n",
        "    print(\"acc = {:.2f}\".format(metrics.f1_score(Y, y_pred,average = 'micro')))\n",
        "    print(\"Precison_Score   : {:.2f}%\".format(precision_score(Y_test,y,average = 'micro')*100))\n",
        "    acc.append(acc)\n",
        "         \n",
        "  \n",
        "  return parameters,costs,acc\n",
        "\n"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJ1fCDE0rs7D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f2cf6b8-07ef-4efb-8129-61563b209633"
      },
      "source": [
        "layers_dims = [15,7,5,1]\n",
        "parameters,costs,acc = L_layer_model(X_train, Y_train, layers_dims, 0.1,num_iterations = 100)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: RuntimeWarning: divide by zero encountered in log\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in multiply\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Cost after iteration 0: nan\n",
            "acc = 0.68\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 1: 1.231280\n",
            "acc = 0.66\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 2: 0.785098\n",
            "acc = 0.64\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 3: 0.662268\n",
            "acc = 0.61\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 4: 0.652337\n",
            "acc = 0.62\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 5: 0.643672\n",
            "acc = 0.61\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 6: 0.635501\n",
            "acc = 0.63\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 7: 0.627925\n",
            "acc = 0.64\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 8: 0.620875\n",
            "acc = 0.65\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 9: 0.614107\n",
            "acc = 0.65\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 10: 0.607761\n",
            "acc = 0.65\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 11: 0.601533\n",
            "acc = 0.67\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 12: 0.595651\n",
            "acc = 0.66\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 13: 0.589900\n",
            "acc = 0.67\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 14: 0.584404\n",
            "acc = 0.68\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 15: 0.578948\n",
            "acc = 0.69\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 16: 0.573656\n",
            "acc = 0.69\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 17: 0.568467\n",
            "acc = 0.70\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 18: 0.563507\n",
            "acc = 0.71\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 19: 0.558702\n",
            "acc = 0.71\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 20: 0.554089\n",
            "acc = 0.71\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 21: 0.549588\n",
            "acc = 0.70\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 22: 0.545246\n",
            "acc = 0.71\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 23: 0.541030\n",
            "acc = 0.73\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 24: 0.536999\n",
            "acc = 0.73\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 25: 0.533120\n",
            "acc = 0.73\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 26: 0.529386\n",
            "acc = 0.74\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 27: 0.525800\n",
            "acc = 0.74\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 28: 0.522296\n",
            "acc = 0.74\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 29: 0.518859\n",
            "acc = 0.75\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 30: 0.515542\n",
            "acc = 0.75\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 31: 0.512322\n",
            "acc = 0.75\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 32: 0.509218\n",
            "acc = 0.76\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 33: 0.506242\n",
            "acc = 0.76\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 34: 0.503371\n",
            "acc = 0.77\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 35: 0.500594\n",
            "acc = 0.78\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 36: 0.497918\n",
            "acc = 0.77\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 37: 0.495358\n",
            "acc = 0.77\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 38: 0.492944\n",
            "acc = 0.77\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 39: 0.490615\n",
            "acc = 0.77\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 40: 0.488368\n",
            "acc = 0.78\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 41: 0.486200\n",
            "acc = 0.78\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 42: 0.484110\n",
            "acc = 0.78\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 43: 0.482126\n",
            "acc = 0.78\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 44: 0.480227\n",
            "acc = 0.78\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 45: 0.478417\n",
            "acc = 0.77\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 46: 0.476672\n",
            "acc = 0.77\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 47: 0.474996\n",
            "acc = 0.77\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 48: 0.473406\n",
            "acc = 0.77\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 49: 0.471879\n",
            "acc = 0.78\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 50: 0.470415\n",
            "acc = 0.77\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 51: 0.469009\n",
            "acc = 0.77\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 52: 0.467654\n",
            "acc = 0.78\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 53: 0.466347\n",
            "acc = 0.78\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 54: 0.465079\n",
            "acc = 0.78\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 55: 0.463842\n",
            "acc = 0.78\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 56: 0.462647\n",
            "acc = 0.78\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 57: 0.461490\n",
            "acc = 0.78\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 58: 0.460371\n",
            "acc = 0.78\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 59: 0.459287\n",
            "acc = 0.78\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 60: 0.458239\n",
            "acc = 0.78\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 61: 0.457225\n",
            "acc = 0.78\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 62: 0.456235\n",
            "acc = 0.78\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 63: 0.455250\n",
            "acc = 0.78\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 64: 0.454281\n",
            "acc = 0.78\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 65: 0.453324\n",
            "acc = 0.78\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 66: 0.452394\n",
            "acc = 0.78\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 67: 0.451454\n",
            "acc = 0.77\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 68: 0.450520\n",
            "acc = 0.77\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 69: 0.449605\n",
            "acc = 0.77\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 70: 0.448714\n",
            "acc = 0.78\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 71: 0.447858\n",
            "acc = 0.78\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 72: 0.447021\n",
            "acc = 0.78\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 73: 0.446204\n",
            "acc = 0.78\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 74: 0.445403\n",
            "acc = 0.78\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 75: 0.444620\n",
            "acc = 0.78\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 76: 0.443853\n",
            "acc = 0.78\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 77: 0.443101\n",
            "acc = 0.78\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 78: 0.442363\n",
            "acc = 0.78\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 79: 0.441633\n",
            "acc = 0.78\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 80: 0.440917\n",
            "acc = 0.78\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 81: 0.440226\n",
            "acc = 0.78\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 82: 0.439544\n",
            "acc = 0.78\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 83: 0.438868\n",
            "acc = 0.78\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 84: 0.438202\n",
            "acc = 0.78\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 85: 0.437546\n",
            "acc = 0.78\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 86: 0.436900\n",
            "acc = 0.77\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 87: 0.436261\n",
            "acc = 0.77\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 88: 0.435617\n",
            "acc = 0.77\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 89: 0.434972\n",
            "acc = 0.77\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 90: 0.434330\n",
            "acc = 0.77\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 91: 0.433683\n",
            "acc = 0.77\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 92: 0.433043\n",
            "acc = 0.77\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 93: 0.432411\n",
            "acc = 0.77\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 94: 0.431786\n",
            "acc = 0.78\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 95: 0.431169\n",
            "acc = 0.79\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 96: 0.430561\n",
            "acc = 0.79\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 97: 0.429959\n",
            "acc = 0.79\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 98: 0.429363\n",
            "acc = 0.79\n",
            "Precison_Score   : 87.50%\n",
            "Cost after iteration 99: 0.428775\n",
            "acc = 0.79\n",
            "Precison_Score   : 87.50%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5Zc9BVthfte",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bc16741-9e0d-43de-c09c-c6d73812dbe7"
      },
      "source": [
        "#Prediction\n",
        "print(X_test.shape,Y_test.shape)\n",
        "AL,_ = forward_model(X_test,parameters)\n",
        "cost = compute_cost(AL,Y_test)\n",
        "\n",
        "y = np.zeros(AL.shape)\n",
        "print(AL)\n",
        "print(np.mean(AL))\n",
        "print(type(y),AL.shape[1])\n",
        "for i in range(0,AL.shape[1]):\n",
        "  y[0][i]  = y[0][i] + sign_f(AL[0][i],np.mean(AL)) \n",
        "print(y)\n",
        "\n",
        "\n",
        "print(\"cost = {:.3f}, acc = {:.3f}\".format(cost,metrics.f1_score(Y_test, y,average = 'micro')))\n",
        "print(\"Precison_Score   : {:.2f}%\".format(precision_score(Y_test,y,average = 'micro')*100))"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(15, 90) (1, 90)\n",
            "[[0.22294335 0.38391126 0.67005422 0.38391126 0.29780392 0.38391126\n",
            "  0.70069277 0.30879993 0.96963972 0.38391126 0.57389547 0.25808238\n",
            "  0.9284523  0.29878071 0.40633505 0.34602916 0.99026602 0.38391126\n",
            "  0.90865185 0.91598984 0.38391126 0.97514529 0.38391126 0.10992479\n",
            "  0.3383586  0.38391126 0.33279789 0.20698787 0.38391126 0.25615749\n",
            "  0.90970368 0.38391126 0.99037688 0.96919894 0.38391126 0.99569433\n",
            "  0.37689066 0.38391126 0.99895342 0.37902926 0.32097818 0.89603296\n",
            "  0.29462386 0.3675159  0.38391126 0.62158807 0.7562321  0.38391126\n",
            "  0.38391126 0.38391126 0.99172403 0.99783688 0.94574368 0.38391126\n",
            "  0.28938267 0.99205528 0.38391126 0.56406465 0.38587499 0.38391126\n",
            "  0.99856201 0.38391126 0.38391126 0.38391126 0.38391126 0.30548859\n",
            "  0.38391126 0.97554449 0.38391126 0.35546244 0.15911222 0.38391126\n",
            "  0.35441991 0.97758775 0.38391126 0.37195492 0.9999159  0.38391126\n",
            "  0.96817441 0.52040483 0.38391126 0.97695176 0.99531202 0.38391126\n",
            "  0.38391126 0.38391126 0.36692414 0.38391126 0.38391126 0.976398  ]]\n",
            "0.5379387261491758\n",
            "<class 'numpy.ndarray'> 90\n",
            "[[0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0.\n",
            "  0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
            "  0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1.]]\n",
            "cost = 0.502, acc = 0.750\n",
            "Precison_Score   : 90.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XP4U1nTGt7mo"
      },
      "source": [
        ""
      ],
      "execution_count": 84,
      "outputs": []
    }
  ]
}